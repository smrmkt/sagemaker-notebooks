{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker のノートブックから Athena を使ってデータの前処理を行う\n",
    "\n",
    "## 概要\n",
    "\n",
    "このノートブックでは、Amazon SageMaker上で Athena を実行して S3 上のデータを読み込み，結果データを pandas に取り込んでさらに処理を行うための例をおみせします．また，より大規模なデータを効率的に前処理するために，非同期な形で Athena を実行するやり方についても説明します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セットアップ\n",
    "\n",
    "### IAM のセットアップ\n",
    "\n",
    "ノートブックインスタンスから Athena に接続できるようにするため，このインスタンスにアタッチしている IAM ロールに，以下のポリシーを付与する\n",
    "\n",
    "- AmazonAthenaFullAccess\n",
    "\n",
    "### テーブルの作成\n",
    "\n",
    "Athena のマネジメントコンソールにいって，クエリエディタ上で以下の DDL を実行する．\n",
    "\n",
    "```\n",
    "CREATE EXTERNAL TABLE default.taxis (\n",
    "     vendorid STRING,\n",
    "     pickup_datetime TIMESTAMP,\n",
    "     dropoff_datetime TIMESTAMP,\n",
    "     ratecode INT,\n",
    "     passenger_count INT,\n",
    "     trip_distance DOUBLE,\n",
    "     fare_amount DOUBLE,\n",
    "     total_amount DOUBLE,\n",
    "     payment_type INT\n",
    "    )\n",
    "PARTITIONED BY (YEAR INT, MONTH INT, TYPE string)\n",
    "STORED AS PARQUET\n",
    "LOCATION 's3://serverless-analytics/canonical/NY-Pub/'\n",
    "```\n",
    "\n",
    "上記テーブルはパーティションに分かれているため，続いて以下のコマンドを実行して，パーティションを認識させる．\n",
    "\n",
    "```\n",
    "MSCK REPAIR TABLE default.taxis\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Athena に対してインタラクティブなクエリを実行\n",
    "\n",
    "### Athena のセットアップ\n",
    "\n",
    "Python から Athena に接続するための PyAthenaJDBC モジュールをインストール．こちらは同期的な処理でクエリを実行するためのツールとなる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyAthenaJDBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S3 から Athena に接続．最初に Athena クエリの結果を吐き出すディレクトリをセットする．デフォルトバケットと揃えるため，以下の `region` と `account_id` に現在のリージョン名とアカウント ID を入力して，Athena との接続をセットアップする．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyathenajdbc import connect\n",
    "\n",
    "region = 'us-east-1' # 別のリージョンで実施する場合は，適宜変更する\n",
    "account_id = '666254511816' #'YOUR-ACCOUNT-ID'\n",
    "\n",
    "bucket_name = 'aws-athena-query-results-{}-{}'.format(account_id, region)\n",
    "staging_dir = 's3://{}'.format(bucket_name)\n",
    "\n",
    "con = connect(s3_staging_dir=staging_dir, region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 結果データを読み出して，ノートブック上で可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "あとは Athena に対してインタラクティブなクエリを実行する．乗車回数および乗客者数の年月推移を可視化してみる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "select\n",
    "  date_trunc('month', pickup_datetime) as date\n",
    "  , count(1) as ride_count\n",
    "  , sum(passenger_count) as passenger_count\n",
    "from\n",
    "  default.taxis\n",
    "where\n",
    "  year is not null\n",
    "group by\n",
    "  date_trunc('month', pickup_datetime)\n",
    "order by\n",
    "  date_trunc('month', pickup_datetime)\n",
    "\"\"\"\n",
    "\n",
    "cursor = con.cursor()\n",
    "cursor.execute(query)\n",
    "data = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得られた結果データを pandas に突っ込んで，matplotlib で可視化してみる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = ['date', 'ride_count', 'passenger_count']\n",
    "df.index = pd.to_datetime(df.date)\n",
    "df = df.drop('date', axis=1)\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 非同期にクエリを実行して結果を S3 に保存\n",
    "\n",
    "上記の PyAthenaJDBC は，あくまでクエリを同期的な形で実行するものであるため，長時間のクエリ実行や，大きなデータの取得には向かない．ノートブックインスタンスでハンドリングするには大きすぎるデータを出力したい場合は，boto3 クライアント経由で，Athena API を直接叩く形で，非同期でのクエリ実行を行うのがよい．結果データは必ず S3 にも保存されるため，あとはそれを入力データとして，機械学習を行えば良い．\n",
    "\n",
    "まずは `start_query_execution` API でクエリを実行．レスポンスを確認してみる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pprint\n",
    "\n",
    "athena = boto3.client('athena', region_name=region)\n",
    "response = athena.start_query_execution(\n",
    "    QueryString = query,\n",
    "    QueryExecutionContext = {\n",
    "        'Database': 'default'\n",
    "    },\n",
    "    ResultConfiguration = {\n",
    "        'OutputLocation': staging_dir,\n",
    "    }\n",
    ")\n",
    "pprint.pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "続いて `get_query_execution` API でクエリの進行状況を確認．ここでは JSON レスポンスのうち，クエリステータスの部分だけ取り出してみた．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "status = 'None'\n",
    "\n",
    "while status != 'SUCCEEDED':\n",
    "    query_status = athena.get_query_execution(\n",
    "        QueryExecutionId=response['QueryExecutionId']\n",
    "    )\n",
    "    status = query_status['QueryExecution']['Status']['State']\n",
    "    print(status)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全体を出力すると，以下のようになる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(query_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クエリ結果の保存先は，`s3://{OutputLocation}/{QueryExecutionId}.csv` となる．今回は，試しに手元に取ってきて，実際に結果が取得できるかを確認してみる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "\n",
    "s3_key = '{}.csv'.format(response['QueryExecutionId'])\n",
    "s3.Bucket(bucket_name).download_file(s3_key, 'output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さっきと同じ結果がローカルにあるかを確認．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat output.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
