{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker で scikit-learn コンテナを使った学習・推論を行う\n",
    "\n",
    "#### ノートブックに含まれる内容\n",
    "\n",
    "- scikit-learn を SageMaker で行うときの，基本的なやりかた\n",
    "\n",
    "#### ノートブックで使われている手法の詳細\n",
    "\n",
    "- アルゴリズム: DecisionTreeClassifier\n",
    "- データ: iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Get a SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習用データを S3 にアップロード\n",
    "\n",
    "SageMaker の学習時につかうデータは，S3 に置く必要があります．ここでは，ローカルにある iris データをいったん SageMaker SDK の `session` クラスにある `upload_data()` メソッドを使って，ノートブックインスタンスのローカルから S3 にアップロードします．\n",
    "\n",
    "デフォルトでは SageMaker は `sagemaker-{region}-{your aws account number}` というバケットを使用します．当該バケットがない場合には，自動で新しく作成します．`upload_data()` メソッドの引数に `bucket=XXXX` という形でデータを配置するバケットを指定することが可能です．\n",
    "\n",
    "以下を実行する前に，**<span style=\"color: red;\">5 行目の `data/scikit-byo-iris/XX` の `XX` を指定された適切な数字に変更</span>**してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn import datasets\n",
    "\n",
    "PREFIX = 'data/scikit-iris/XX'\n",
    "\n",
    "# Load Iris dataset, then join labels and features\n",
    "iris = datasets.load_iris()\n",
    "joined_iris = np.insert(iris.data, 0, iris.target, axis=1)\n",
    "\n",
    "# Create directory and write csv\n",
    "os.makedirs('./data', exist_ok=True)\n",
    "np.savetxt('./data/iris.csv', joined_iris, delimiter=',', fmt='%1.1f, %1.3f, %1.3f, %1.3f, %1.3f')\n",
    "\n",
    "train_input = sagemaker_session.upload_data('data', key_prefix=PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの学習を実行\n",
    "\n",
    "SageMaker で学習を行うために，SageMaker SDK で `Estimator` オブジェクトをつくります．このオブジェクトには，学習をおこなうために以下の設定が含まれます．その上で，`fit()` メソッドで学習を実施します．学習には 5 分程度時間がかかります．\n",
    "\n",
    "* __role__: ジョブを実行する IAM role\n",
    "* __instance count__:  学習ジョブに使うインスタンス数\n",
    "* __instance type__ 学習ジョブに使うインスタンスタイプ\n",
    "* __output path__: 学習の成果物が置かれる S3 の場所\n",
    "* __session__: すぐ上で作成した，SageMaker セッション\n",
    "\n",
    "scikit-learn コンテナを使う場合，基本的にはスクリプトの `__main__` 関数内に，学習処理をベタ書きすれば OK です．その際に，入力データの場所やモデルファイルを出力する場所などは，環境変数として SageMaker から引き渡されます．具体的な環境変数の一覧は以下のとおりです．\n",
    "\n",
    "* `SM_MODEL_DIR`: 出力モデルを配置する，コンテナ内のディレクトリのパスをさします．このパスにモデルファイルを出力しておけば，SageMaker が学習終了時にフォルダの中身を tar.gz にまとめて，S3 に出力してくれます．\n",
    "* `SM_OUTPUT_DIR`: モデルファイル以外の出力ファイルを置くためのディレクトリパスです．こちらも同様に，SageMaker が S3 にデータを出力します．\n",
    "* `SM_CHANNEL_TRAIN`: `fit()` を実行する際に指定するデータのうち，`train` タグのついた学習用データが置かれる，コンテナ内のディレクトリパスをさします\n",
    "* `SM_CHANNEL_TEST`: 上と同様に，`test` タグのついた検証用データのパスをさします．\n",
    "\n",
    "また `Estimator` オブジェクト作成時に，Hyperparameter として指定したものは，引数としてスクリプトに渡ってくるので，argparse パッケージを用いて取得可能です．\n",
    "こちらは `scilit_learn_iris.py` をご覧ください．\n",
    "\n",
    "もし追加のモジュールインストールが必要な場合には，`source_dir` 直下に `requirements.txt` を配置することで，コンテナ起動時にインストールされます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "script_path = 'scikit_learn_iris.py'\n",
    "source_dir='src/'\n",
    "\n",
    "sklearn = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    source_dir=source_dir,\n",
    "    train_instance_type=\"ml.m4.xlarge\",\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters={'max_leaf_nodes': 10})\n",
    "\n",
    "sklearn.fit({'train': train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの推論を実行\n",
    "\n",
    "推論を行うために，まず学習したモデルをデプロイします．`deploy()` メソッドでは，デプロイ先エンドポイントのインスタンス数，インスタンスタイプを指定します．また併せて，オプションで（リクエストで渡されるデータの）シリアライザと（レスポンスで返されるデータの）デシリアライザを指定することも可能です．モデルのデプロイには 10 分程度時間がかかります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sklearn.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 推論用のデータを準備\n",
    "test_X = pd.DataFrame([[5.0, 3.2, 1.2, 4.3], [4.5, 2.3, 1.3, 0.3], [5.7, 2.8, 4.1, 1.3]])\n",
    "\n",
    "print(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論を実行して，結果を表示\n",
    "result = predictor.predict(test_X.values)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エンドポイントの削除\n",
    "\n",
    "全て終わったら，エンドポイントを削除します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
