{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object2Vec を用いたドキュメントの分散表現取得\n",
    "\n",
    "#### ノートブックに含まれる内容\n",
    "\n",
    "- Object2Vec の使い方\n",
    "\n",
    "#### ノートブックで使われている手法の詳細\n",
    "\n",
    "- アルゴリズム: Object2Vec\n",
    "- データ: Wikipedia データ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このノートブックでは，Object2Vec でドキュメントの分散表現を獲得する学習と推論を行います．Object2Vec は，2 種類のインプットをとります．その 2 種類のインプットをそれぞれ分散表現に落とした上で，それらを比較して最終的なスコア / ラベルを予測します．インプットとして，文章や単語，ID 等を自由に取ることが可能です．アルゴリズム概要については，[Amazon SageMaker Object2Vec の概要](https://aws.amazon.com/jp/blogs/news/introduction-to-amazon-sagemaker-object2vec/)をご覧ください．\n",
    "\n",
    "ここでは，Wikipedia のドキュメントをもとに，ドキュメント内のあるセンテンスと，ドキュメントから当該センテンスを抜いたもののペアを入力とします．このペアに対しては，正例のラベル 1 を付与します．学習用に準備するデータは，正例データのみです．Object2Vec の学習時の`negative_sampling_rate` を 1 以上の整数値にすることで，1 つの正例サンプルに対して指定した数ぶんの負例サンプルが学習時に自動生成されます．負例データは，センテンスとドキュメントをランダムに組み合わせたものとなります．\n",
    "\n",
    "これを図で表すと，以下の通りになります．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"doc_embedding_illustration.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object2Vec を分散表現を獲得するために用いる場合は，パフォーマンス向上のために，他に疎行列による勾配の更新，Embedding レイヤーの重みのエンコーダ間共有，また比較演算子のカスタマイズといったいくつかのハイパーパラメタを利用することが可能です．詳細は[こちらのブログ](https://aws.amazon.com/jp/blogs/news/amazon-sagemaker-object2vec-adds-new-features-that-support-automatic-negative-sampling-and-speed-up-training/)をご覧ください．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facebook AI が提供している Wikipedia のコーパスデータを用いて，Object2Vec 用に加工します．以下，前処理のスニペットが多く並びますが，単に Object2Vec の入力形式に整形しているだけです．入力形式については，[こちら](https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/object2vec-training-formats.html)にまとまっている通りです．`in0` は `encoder0` の入力，`in1` は `encoder1` の入力になります．ここでは，センテンスとドキュメントを語彙ファイルで数値 ID にエンコードしなおした配列が，それぞれのエンコーダーに与えられます．\n",
    "\n",
    "```\n",
    "{\"label\": 1, \"in0\": [774, 14, 21, 206], \"in1\": [21, 366, 125]}\n",
    "{\"label\": 1, \"in0\": [236, 4, 227, 391, 521, 881], \"in1\": [31, 1369]}\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "DATANAME=\"wikipedia\"\n",
    "DATADIR=\"/tmp/wiki\"\n",
    "\n",
    "mkdir -p \"${DATADIR}\"\n",
    "\n",
    "if [ ! -f \"${DATADIR}/${DATANAME}_train250k.txt\" ]\n",
    "then\n",
    "    echo \"Downloading wikipedia data\"\n",
    "    wget --quiet -c \"https://dl.fbaipublicfiles.com/starspace/wikipedia_train250k.tgz\" -O \"${DATADIR}/${DATANAME}_train.tar.gz\"\n",
    "    tar -xzvf \"${DATADIR}/${DATANAME}_train.tar.gz\" -C \"${DATADIR}\"\n",
    "    wget --quiet -c \"https://dl.fbaipublicfiles.com/starspace/wikipedia_devtst.tgz\" -O \"${DATADIR}/${DATANAME}_test.tar.gz\"\n",
    "    tar -xzvf \"${DATADIR}/${DATANAME}_test.tar.gz\" -C \"${DATADIR}\"\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '/tmp/wiki'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /tmp/wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: please run on python 3 kernel\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import math\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "import json, jsonlines\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from itertools import chain, islice\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "## sagemaker api\n",
    "import sagemaker, boto3\n",
    "from sagemaker.session import s3_input\n",
    "from sagemaker.predictor import json_serializer, json_deserializer\n",
    "\n",
    "sess = sagemaker.session.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_SYMBOL = \"<s>\"\n",
    "EOS_SYMBOL = \"</s>\"\n",
    "UNK_SYMBOL = \"<unk>\"\n",
    "PAD_SYMBOL = \"<pad>\"\n",
    "PAD_ID = 0\n",
    "TOKEN_SEPARATOR = \" \"\n",
    "VOCAB_SYMBOLS = [PAD_SYMBOL, UNK_SYMBOL, BOS_SYMBOL, EOS_SYMBOL]\n",
    "\n",
    "\n",
    "##### utility functions for preprocessing\n",
    "def get_article_iter_from_file(fname):\n",
    "    with open(fname) as f:\n",
    "        for article in f:\n",
    "            yield article\n",
    "\n",
    "def get_article_iter_from_channel(channel, datadir='/tmp/wiki'):\n",
    "    if channel == 'train':\n",
    "        fname = os.path.join(datadir, 'wikipedia_train250k.txt')\n",
    "        return get_article_iter_from_file(fname)\n",
    "    else:\n",
    "        iterlist = []\n",
    "        suffix_list = ['train250k.txt', 'test10k.txt', 'dev10k.txt', 'test_basedocs.txt']\n",
    "        for suffix in suffix_list:\n",
    "            fname = os.path.join(datadir, 'wikipedia_'+suffix)\n",
    "            iterlist.append(get_article_iter_from_file(fname))\n",
    "        return chain.from_iterable(iterlist)\n",
    "\n",
    "\n",
    "def readlines_from_article(article):\n",
    "    return article.strip().split('\\t')\n",
    "\n",
    "\n",
    "def sentence_to_integers(sentence, word_dict, trim_size=None):\n",
    "    \"\"\"\n",
    "    Converts a string of tokens to a list of integers\n",
    "    \"\"\"\n",
    "    if not trim_size:\n",
    "        return [word_dict[token] if token in word_dict else 0 for token in get_tokens_from_sentence(sentence)]\n",
    "    else:\n",
    "        integer_list = []\n",
    "        for token in get_tokens_from_sentence(sentence):\n",
    "            if len(integer_list) < trim_size:\n",
    "                if token in word_dict:\n",
    "                    integer_list.append(word_dict[token])\n",
    "                else:\n",
    "                    integer_list.append(0)\n",
    "            else:\n",
    "                break\n",
    "        return integer_list\n",
    "\n",
    "\n",
    "def get_tokens_from_sentence(sent):\n",
    "    \"\"\"\n",
    "    Yields tokens from input string.\n",
    "\n",
    "    :param line: Input string.\n",
    "    :return: Iterator over tokens.\n",
    "    \"\"\"\n",
    "    for token in sent.split():\n",
    "        if len(token) > 0:\n",
    "            yield normalize_token(token)\n",
    "\n",
    "\n",
    "def get_tokens_from_article(article):\n",
    "    iterlist = []\n",
    "    for sent in readlines_from_article(article):\n",
    "        iterlist.append(get_tokens_from_sentence(sent))\n",
    "    return chain.from_iterable(iterlist)\n",
    "\n",
    "\n",
    "def normalize_token(token):\n",
    "    token = token.lower()\n",
    "    if all(s.isdigit() or s in string.punctuation for s in token):\n",
    "        tok = list(token)\n",
    "        for i in range(len(tok)):\n",
    "            if tok[i].isdigit():\n",
    "                tok[i] = '0'\n",
    "        token = \"\".join(tok)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build vocabulary\n",
    "\n",
    "def build_vocab(channel, num_words=50000, min_count=1, use_reserved_symbols=True, sort=True):\n",
    "    \"\"\"\n",
    "    Creates a vocabulary mapping from words to ids. Increasing integer ids are assigned by word frequency,\n",
    "    using lexical sorting as a tie breaker. The only exception to this are special symbols such as the padding symbol\n",
    "    (PAD).\n",
    "\n",
    "    :param num_words: Maximum number of words in the vocabulary.\n",
    "    :param min_count: Minimum occurrences of words to be included in the vocabulary.\n",
    "    :return: word-to-id mapping.\n",
    "    \"\"\"\n",
    "    vocab_symbols_set = set(VOCAB_SYMBOLS)\n",
    "    raw_vocab = Counter()\n",
    "    for article in get_article_iter_from_channel(channel):\n",
    "        article_wise_vocab_list = list()\n",
    "        for token in get_tokens_from_article(article):\n",
    "            if token not in vocab_symbols_set:\n",
    "                article_wise_vocab_list.append(token)\n",
    "        raw_vocab.update(article_wise_vocab_list)\n",
    "\n",
    "    print(\"Initial vocabulary: {} types\".format(len(raw_vocab)))\n",
    "\n",
    "    # For words with the same count, they will be ordered reverse alphabetically.\n",
    "    # Not an issue since we only care for consistency\n",
    "    pruned_vocab = sorted(((c, w) for w, c in raw_vocab.items() if c >= min_count), reverse=True)\n",
    "    print(\"Pruned vocabulary: {} types (min frequency {})\".format(len(pruned_vocab), min_count))\n",
    "\n",
    "    # truncate the vocabulary to fit size num_words (only includes the most frequent ones)\n",
    "    vocab = islice((w for c, w in pruned_vocab), num_words)\n",
    "\n",
    "    if sort:\n",
    "        # sort the vocabulary alphabetically\n",
    "        vocab = sorted(vocab)\n",
    "    if use_reserved_symbols:\n",
    "        vocab = chain(VOCAB_SYMBOLS, vocab)\n",
    "\n",
    "    word_to_id = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "    print(\"Final vocabulary: {} types\".format(len(word_to_id)))\n",
    "\n",
    "    if use_reserved_symbols:\n",
    "        # Important: pad symbol becomes index 0\n",
    "        assert word_to_id[PAD_SYMBOL] == PAD_ID\n",
    "    \n",
    "    return word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocab dictionary\n",
    "\n",
    "def build_vocabulary_file(vocab_fname, channel, num_words=50000, min_count=1, \n",
    "                          use_reserved_symbols=True, sort=True, force=False):\n",
    "    if not os.path.exists(vocab_fname) or force:\n",
    "        w_dict = build_vocab(channel, num_words=num_words, min_count=min_count, \n",
    "                             use_reserved_symbols=True, sort=True)\n",
    "        with open(vocab_fname, \"w\") as write_file:\n",
    "            json.dump(w_dict, write_file)\n",
    "\n",
    "channel = 'train'\n",
    "min_count = 5\n",
    "vocab_fname = os.path.join(datadir, 'wiki-vocab-{}250k-mincount-{}.json'.format(channel, min_count))\n",
    "\n",
    "build_vocabulary_file(vocab_fname, channel, num_words=500000, min_count=min_count, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading vocab file {} ...\".format(vocab_fname))\n",
    "\n",
    "with open(vocab_fname) as f:\n",
    "    w_dict = json.load(f)\n",
    "    print(\"The vocabulary size is {}\".format(len(w_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to build training data \n",
    "# Tokenize wiki articles to (sentence, document) pairs\n",
    "def generate_sent_article_pairs_from_single_article(article, word_dict):\n",
    "    sent_list = readlines_from_article(article)\n",
    "    art_len = len(sent_list)\n",
    "    idx = random.randint(0, art_len-1)\n",
    "    wrapper_text_idx = list(range(idx)) + list(range((idx+1) % art_len, art_len))\n",
    "    wrapper_text_list = sent_list[:idx] + sent_list[(idx+1) % art_len : art_len]\n",
    "    wrapper_tokens = []\n",
    "    for sent1 in wrapper_text_list:\n",
    "        wrapper_tokens += sentence_to_integers(sent1, word_dict)\n",
    "    sent_tokens = sentence_to_integers(sent_list[idx], word_dict)\n",
    "    yield {'in0':sent_tokens, 'in1':wrapper_tokens, 'label':1}\n",
    "\n",
    "\n",
    "def generate_sent_article_pairs_from_single_file(fname, word_dict):\n",
    "    with open(fname) as reader:\n",
    "        iter_list = []\n",
    "        for article in reader:\n",
    "            iter_list.append(generate_sent_article_pairs_from_single_article(article, word_dict))\n",
    "    return chain.from_iterable(iter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build training data\n",
    "\n",
    "# Generate integer positive labeled data\n",
    "train_prefix = 'train250k'\n",
    "fname = \"wikipedia_{}.txt\".format(train_prefix)\n",
    "outfname = os.path.join(datadir, '{}_tokenized.jsonl'.format(train_prefix))\n",
    "counter = 0\n",
    "\n",
    "with jsonlines.open(outfname, 'w') as writer:\n",
    "    for sample in generate_sent_article_pairs_from_single_file(os.path.join(datadir, fname), w_dict):\n",
    "        writer.write(sample)\n",
    "        counter += 1\n",
    "        \n",
    "print(\"Finished generating {} data of size {}\".format(train_prefix, counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle training data\n",
    "!shuf {outfname} > {train_prefix}_tokenized_shuf.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to generate dev/test data (with both positive and negative labels)\n",
    "\n",
    "def generate_pos_neg_samples_from_single_article(word_dict, article_idx, article_buffer, negative_sampling_rate=1):\n",
    "    sample_list = []\n",
    "    # generate positive samples\n",
    "    sent_list = readlines_from_article(article_buffer[article_idx])\n",
    "    art_len = len(sent_list)\n",
    "    idx = random.randint(0, art_len-1)\n",
    "    wrapper_text_idx = list(range(idx)) + list(range((idx+1) % art_len, art_len))\n",
    "    wrapper_text_list = sent_list[:idx] + sent_list[(idx+1) % art_len : art_len]\n",
    "    wrapper_tokens = []\n",
    "    for sent1 in wrapper_text_list:\n",
    "        wrapper_tokens += sentence_to_integers(sent1, word_dict)\n",
    "    sent_tokens = sentence_to_integers(sent_list[idx], word_dict)\n",
    "    sample_list.append({'in0':sent_tokens, 'in1':wrapper_tokens, 'label':1})\n",
    "    # generate negative sample\n",
    "    buff_len = len(article_buffer)\n",
    "    sampled_inds = np.random.choice(list(range(article_idx)) + list(range((article_idx+1) % buff_len, buff_len)), \n",
    "                                    size=negative_sampling_rate)\n",
    "    for n_idx in sampled_inds:\n",
    "        other_article = article_buffer[n_idx]\n",
    "        context_list = readlines_from_article(other_article)\n",
    "        context_tokens = []\n",
    "        for sent2 in context_list:\n",
    "            context_tokens += sentence_to_integers(sent2, word_dict)\n",
    "        sample_list.append({'in0': sent_tokens, 'in1':context_tokens, 'label':0})\n",
    "    return sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dev and test data\n",
    "for data in ['dev10k', 'test10k']:\n",
    "    fname = os.path.join(datadir,'wikipedia_{}.txt'.format(data))\n",
    "    test_nsr = 5\n",
    "    outfname = '{}_tokenized-nsr{}.jsonl'.format(data, test_nsr)\n",
    "    article_buffer = list(get_article_iter_from_file(fname))\n",
    "    sample_buffer = []\n",
    "    for article_idx in range(len(article_buffer)):\n",
    "        sample_buffer += generate_pos_neg_samples_from_single_article(w_dict, article_idx, \n",
    "                                                                      article_buffer, \n",
    "                                                                      negative_sampling_rate=test_nsr)\n",
    "    with jsonlines.open(outfname, 'w') as writer:\n",
    "        writer.write_all(sample_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これで学習用，テスト用のデータセットが作成できたので，中身を確認してみましょう．`in0` にセンテンスが，`in1` にドキュメントが，それぞれ数値エンコードされた配列として格納されており，そのあとに正例である 1 のラベルが付与された jsonl の形式です．冒頭で述べたように，アルゴリズム内で自動生成されるため，学習データに負例データは含まれていません．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 3 train250k_tokenized_shuf.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA=\"train250k_tokenized_shuf.jsonl\"\n",
    "DEV_DATA=\"dev10k_tokenized-nsr{}.jsonl\".format(test_nsr)\n",
    "TEST_DATA=\"test10k_tokenized-nsr{}.jsonl\".format(test_nsr)\n",
    "\n",
    "# NOTE: define your s3 bucket and key here\n",
    "S3_BUCKET = sess.default_bucket()\n",
    "S3_KEY = 'object2vec-doc2vec'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$TRAIN_DATA\" \"$DEV_DATA\" \"$TEST_DATA\" \"$S3_BUCKET\" \"$S3_KEY\"\n",
    "\n",
    "aws s3 cp \"$1\" s3://$4/$5/input/train/\n",
    "aws s3 cp \"$2\" s3://$4/$5/input/validation/\n",
    "aws s3 cp \"$3\" s3://$4/$5/input/test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "print(\"Your notebook is running on region '{}'\".format(region))\n",
    " \n",
    "role = get_execution_role()\n",
    "print(\"Your IAM role: '{}'\".format(role))\n",
    "\n",
    "container = get_image_uri(region, 'object2vec')\n",
    "print(\"The image uri used is '{}'\".format(container))\n",
    "\n",
    "print(\"Using s3 buceket: {} and key prefix: {}\".format(S3_BUCKET, S3_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define input channels\n",
    "\n",
    "s3_input_path = os.path.join('s3://', S3_BUCKET, S3_KEY, 'input')\n",
    "\n",
    "s3_train = s3_input(os.path.join(s3_input_path, 'train', TRAIN_DATA), \n",
    "                    distribution='ShardedByS3Key', content_type='application/jsonlines')\n",
    "\n",
    "s3_valid = s3_input(os.path.join(s3_input_path, 'validation', DEV_DATA), \n",
    "                    distribution='ShardedByS3Key', content_type='application/jsonlines')\n",
    "\n",
    "s3_test = s3_input(os.path.join(s3_input_path, 'test', TEST_DATA), \n",
    "                   distribution='ShardedByS3Key', content_type='application/jsonlines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define output path\n",
    "output_path = os.path.join('s3://', S3_BUCKET, S3_KEY, 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの学習を実行\n",
    "\n",
    "ハイパーパラメタを以下のように設定して，モデルの学習を実行します．学習には 15 分ほどかかります．ハイパーパラメタの詳細については，[こちら](https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/object2vec-hyperparameters.html)をご参照ください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training hyperparameters\n",
    "\n",
    "hyperparameters = {\n",
    "      \"_kvstore\": \"device\",\n",
    "      \"_num_gpus\": 'auto',\n",
    "      \"_num_kv_servers\": \"auto\",\n",
    "      \"bucket_width\": 0,\n",
    "      \"dropout\": 0.4,\n",
    "      \"early_stopping_patience\": 2,\n",
    "      \"early_stopping_tolerance\": 0.01,\n",
    "      \"enc0_layers\": \"auto\",\n",
    "      \"enc0_max_seq_len\": 50,\n",
    "      \"enc0_network\": \"pooled_embedding\",\n",
    "      \"enc0_pretrained_embedding_file\": \"\",\n",
    "      \"enc0_token_embedding_dim\": 300,\n",
    "      \"enc0_vocab_size\": 267522,\n",
    "      \"enc1_network\": \"enc0\",\n",
    "      \"enc_dim\": 300,\n",
    "      \"epochs\": 20,\n",
    "      \"learning_rate\": 0.01,\n",
    "      \"mini_batch_size\": 512,\n",
    "      \"mlp_activation\": \"relu\",\n",
    "      \"mlp_dim\": 512,\n",
    "      \"mlp_layers\": 2,\n",
    "      \"num_classes\": 2,\n",
    "      \"optimizer\": \"adam\",\n",
    "      \"output_layer\": \"softmax\",\n",
    "      \"weight_decay\": 0\n",
    "}\n",
    "\n",
    "\n",
    "hyperparameters['negative_sampling_rate'] = 3\n",
    "hyperparameters['tied_token_embedding_weight'] = \"true\"\n",
    "hyperparameters['comparator_list'] = \"hadamard\"\n",
    "hyperparameters['token_embedding_storage_type'] = 'row_sparse'\n",
    "\n",
    "    \n",
    "# get estimator\n",
    "doc2vec = sagemaker.estimator.Estimator(container,\n",
    "                                          role, \n",
    "                                          train_instance_count=1, \n",
    "                                          train_instance_type='ml.p2.xlarge',\n",
    "                                          output_path=output_path,\n",
    "                                          sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "doc2vec.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "# fit estimator with data\n",
    "doc2vec.fit({'train': s3_train, 'validation':s3_valid, 'test':s3_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの推論を実行\n",
    "\n",
    "推論を行うために，Estimateor オブジェクトからモデルオブジェクトを作成した上で，モデルをエンドポイントにデプロイします．deploy() メソッドでは，デプロイ先エンドポイントのインスタンス数，インスタンスタイプを指定します．モデルのデプロイには 10 分程度時間がかかります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy model\n",
    "\n",
    "doc2vec_model = doc2vec.create_model(\n",
    "                        serializer=json_serializer,\n",
    "                        deserializer=json_deserializer,\n",
    "                        content_type='application/json')\n",
    "\n",
    "predictor = doc2vec_model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エンドポイントができたら，テスト用のデータを準備して，実際に分散表現を取得してみましょう．分散表現を取得する際には，以下のようなデータをリクエストボディとして準備します．`in0` であれば `encoder0` が，`in1` であれば `encoder1` が，分散表現の獲得のために使用されます．ただし今回は，`tied_token_embedding_weight` を `True` にしており，`encoder0` と `encoder1` は共通のものが使われるため，`in0` でも `in1` でも帰ってくる結果は同一のものになります．\n",
    "\n",
    "ここでは推論に json フォーマットを使用していますが，jsonlines フォーマットを使用することも可能です．詳細については，[こちら](https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/object2vec-encoder-embeddings.html)をご覧ください．\n",
    "\n",
    "```\n",
    "{\n",
    "  \"instances\" : [\n",
    "    {\"in0\": [6, 17, 606, 19, 53, 67, 52, 12, 5, 10, 15, 10178, 7, 33, 652, 80, 15, 69, 821, 4]},\n",
    "    {\"in0\": [22, 1016, 32, 13, 25, 11, 5, 64, 573, 45, 5, 80, 15, 67, 21, 7, 9, 107, 4]},\n",
    "    {\"in0\": [774, 14, 21, 206]}\n",
    "  ]\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embeddings for sentence and ground-truth article pairs\n",
    "sentences = []\n",
    "gt_articles = []\n",
    "pairs = []\n",
    "for data in read_jsonline(test_fpath):\n",
    "    if data['label'] == 1:\n",
    "        sentences.append({'in0': data['in0']})\n",
    "        gt_articles.append({'in0': data['in1']})\n",
    "        pairs.append({'in0': data['in0'], 'in1': data['in1']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'instances': sentences[:3]}\n",
    "response = send_payload(predictor, payload)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また以下のように `in0` と `in1` を両方入力データに含めることで，1/0 ラベルの推論結果を取得することが可能になります．\n",
    "\n",
    "```\n",
    "{\n",
    "  \"instances\" : [\n",
    "    {\"in0\": [774, 14, 21, 206], \"in1\": [21, 366, 125]}\n",
    "    {\"in0\": [236, 4, 227, 391, 521, 881], \"in1\": [31, 1369]}\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'instances': pairs[:3]}\n",
    "response = send_payload(predictor, payload)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エンドポイントの削除\n",
    "\n",
    "全て終わったら，エンドポイントを削除します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
