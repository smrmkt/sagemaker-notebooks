{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker で提供される Neural Topic Model の学習と推論を行う\n",
    "\n",
    "#### ノートブックに含まれる内容\n",
    "\n",
    "- Amazon が提供するアルゴリズムの使いかた\n",
    "- 中でも，Neural Topic Model の使い方\n",
    "\n",
    "#### ノートブックで使われている手法の詳細\n",
    "\n",
    "- アルゴリズム: Neural Topic Model\n",
    "- データ: スクリプトで自動生成されたサンプルデータ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## セットアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "prefix = 'sagemaker/DEMO-ntm-synthetic'\n",
    " \n",
    "import boto3\n",
    "import re\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from generate_example_data import generate_griffiths_data, plot_topic_data\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import scipy\n",
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "sess = sagemaker.session.Session()\n",
    "bucket = sess.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの生成\n",
    "\n",
    "ここでは `generate_example_data.py` を使って，学習用のデータを生成します．文章軍に含まれ全単語のボキャブラリー数，ドキュメント数およびトピック数を指定して，ディリクレ分布にしたがってサンプルデータを生成します．ここで各単語は具体的な文字列ではなく，あらかじめ数字にエンコードされた形であらわされます．SageMaker の Neural Topic Model では，学習用データに単語や文章をそのまま使うことはできません．ドキュメント行 x ボキャブラリー列の行列形式で学習用データを用意する必要があります．各セルの中身は，当該ドキュメントにおける，当該単語の出現回数となります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the sample data\n",
    "vocabulary_size = 25\n",
    "num_documents = 5000\n",
    "num_topics = 5\n",
    "\n",
    "known_alpha, known_beta, documents, topic_mixtures = generate_griffiths_data(\n",
    "    num_documents=num_documents, num_topics=num_topics, vocabulary_size=vocabulary_size)\n",
    "\n",
    "# separate the generated data into training and tests subsets\n",
    "num_documents_training = int(0.8*num_documents)\n",
    "num_documents_test = num_documents - num_documents_training\n",
    "\n",
    "documents_training = documents[:num_documents_training]\n",
    "documents_test = documents[num_documents_training:]\n",
    "\n",
    "topic_mixtures_training = topic_mixtures[:num_documents_training]\n",
    "topic_mixtures_test = topic_mixtures[num_documents_training:]\n",
    "\n",
    "data_training = (documents_training, np.zeros(num_documents_training))\n",
    "data_test = (documents_test, np.zeros(num_documents_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データの生成が終わったら，実際に中身を確認してみましょう．最初のドキュメントが，25 種類の単語の生起回数で表現されているのが確認できます．25 は，上で指定した `vocabulary_size` がそのまま使われています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First training document = {}'.format(documents[0]))\n",
    "print('\\nVocabulary size = {}'.format(vocabulary_size))\n",
    "print('Shape of training data = {}'.format(documents.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また併せて，最初のドキュメントに含まれる各トピックの割合をみてみます．トピック数は同じく，上で指定した `num_topics` が使われています．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print('Known topic mixture of first training document = {}'.format(topic_mixtures_training[0]))\n",
    "print('\\nNumber of topics = {}'.format(num_topics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また，最初の 10 ドキュメントについて，25 種類の単語がそれぞれ何回含まれているかを可視化してみると，以下のように表現できます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig = plot_topic_data(documents_training[:10], nrows=2, ncols=5, cmap='gray_r', with_colorbar=False)\n",
    "fig.suptitle('Example Documents')\n",
    "fig.set_dpi(160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データのロード\n",
    "\n",
    "SageMaker の学習時につかうデータは，S3 に置く必要があります．ここでは，データを SageMaker SDK が用意しているメソッドを用いて，RecordIO フォーマットに変換します．その上で S3 にアップロードする形をとります．`write_numpy_to_dense_tensor` については[こちら](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/amazon/common.py#L88-L110)を参照ください．なお，ここではパフォーマンスが得られる RecordIO 形式に直していますが，CSV 形式のままアップロードして学習を行うことも可能です．詳細は[こちら](https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/ntm.html#NTM-inputoutput)を参照ください．\n",
    "\n",
    "ここでは自動生成データのため，ボキャブラリーと実際の単語のマッピングを行なっていませんが，実際に使用する際には，ボキャブラリー内のこの整数 ID で示される単語はこれだ，というマッピングを行いたいことがあるかと思います．その際には，補助語彙チャネルとして vocab.txt というファイルを用意して，学習時に読み込ませることで，マッピングを行うことができます．詳細については，[こちらのブログ記事](https://aws.amazon.com/jp/blogs/news/amazon-sagemaker-neural-topic-model-now-supports-auxiliary-vocabulary-channel-new-topic-evaluation-metrics-and-training-subsampling/)を参照ください．この補助語彙チャネルを用いることで，単語埋め込みトピックコヒーレンスメトリクス (WETC) やトピック一貫性メトリクス (TU) を利用することも可能となります．ざっくりまとめると，WETC は同一トピック内の単語の類似性を 0-1 で，TU はトピック内の単語が他のトピックにどれだけ現れるかを (1/K)-1 (K はトピック数)で表す指標になります．ともに高いほど，トピックがうまく分割できていることを示します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, data_training[0].astype('float32'))\n",
    "buf.seek(0)\n",
    "\n",
    "key = 'ntm.data'\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', key)).upload_fileobj(buf)\n",
    "s3_train_data = 's3://{}/{}/train/{}'.format(bucket, prefix, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの学習を実行\n",
    "\n",
    "データの準備ができたら，さっそく学習ジョブを実行しましょう．ここでは，SageMaker SDK で用意されている関数を使って，ビルトインアルゴリズムのコンテナイメージ ID を取得します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'ntm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Topic Model では，以下の　2 つのハイパーパラメタを指定できます．\n",
    "\n",
    "* **`num_topics`** - 推定するトピック数．ここでは，ディリクレ分布で生成した際のトピック数である 5 をそのまま使用します．\n",
    "\n",
    "* **`feature_dim`** - ボキャブラリーの数を指定します．こちらも上で指定した単語数 25 をそのまま用います．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.c4.xlarge',\n",
    "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                                    sagemaker_session=sess)\n",
    "ntm.set_hyperparameters(num_topics=num_topics,\n",
    "                        feature_dim=vocabulary_size)\n",
    "\n",
    "ntm.fit({'train': s3_train_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの推論を実行\n",
    "\n",
    "推論を行うために，Estimateor オブジェクトからモデルオブジェクトを作成した上で，モデルをエンドポイントにデプロイします．deploy() メソッドでは，デプロイ先エンドポイントのインスタンス数，インスタンスタイプを指定します．モデルのデプロイには 10 分程度時間がかかります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm_predictor = ntm.deploy(initial_instance_count=1,\n",
    "                           instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "デプロイが終わったら，実際に推論を行ってみましょう．学習で使ったのと同じ形式のデータをリクエストで送ることで，5 つのトピックがそれぞれ含まれる確率を，レスポンスとして得ることができます．ここではシリアライザを csv に，でシリアライザを　json に指定しています．これによりリクエストで送られる nparray データを csv 形式に変換して推論エンドポイントに送り，またレスポンスを json に変換して受け取ることができます．推論形式の詳細については，[こちら](https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/cdf-inference.html) を参照ください．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm_predictor.content_type = 'text/csv'\n",
    "ntm_predictor.serializer = csv_serializer\n",
    "ntm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ntm_predictor.predict(documents_training[:10])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の結果は，以下のような形式の json データです．このままだと見辛いので，少し加工して，見やすくしてみましょう．\n",
    "```\n",
    "{\n",
    "  'predictions': [\n",
    "    {'topic_weights': [ ... ] },\n",
    "    {'topic_weights': [ ... ] },\n",
    "    {'topic_weights': [ ... ] },\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.array([prediction['topic_weights'] for prediction in results['predictions']])\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同じ文章について，おおもとのトピック分布と，推定されたトピック分布を比べてみましょう．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_mixtures_training[0])  # known topic mixture\n",
    "print(predictions[0])  # computed topic mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを 1000 行ずつ投げて推論をしてみましょう．エンドポイントのリクエストボディは 5MB が上限なので，それより小さなサイズになるように，リクエストデータをうまく分割する必要があります．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batches(data, rows=1000):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = []\n",
    "    for array in split_array:\n",
    "        results = ntm_predictor.predict(array)\n",
    "        predictions += [r['topic_weights'] for r in results['predictions']]\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_batches(documents_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エンドポイントの削除\n",
    "\n",
    "全て終わったら，エンドポイントを削除します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(ntm_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
